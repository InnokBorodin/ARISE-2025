{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import models\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(joint_type_and_param: str) -> tuple(torch.utils.data.DataLoader, torch.utils.data.DataLoader, int):\n",
    "    '''\n",
    "    Make 2 DataLoaders (using torchvision.datasets.ImageFolder) and count number of classes.\n",
    "\n",
    "    Args:\n",
    "        joint_type_and_param (str): f\"{joint_type}_{param}\", for example \"ulna_erosion\"\n",
    "\n",
    "    Return:\n",
    "        train_dataloader\n",
    "        val_dataloader\n",
    "        n_classes (int): number of classes\n",
    "    '''\n",
    "    mean_list, std_list = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "    train_transform = v2.Compose([\n",
    "        v2.Resize((224, 224)),\n",
    "        v2.RandomRotation(15),\n",
    "        v2.RandomHorizontalFlip(p = 0.3),\n",
    "        v2.RandomVerticalFlip(p = 0.3),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean_list, std_list)\n",
    "    ])\n",
    "\n",
    "    val_transform = v2.Compose([\n",
    "        v2.Resize((224, 224)),\n",
    "        v2.RandomRotation(15),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean_list, std_list)\n",
    "    ])\n",
    "\n",
    "    data_root = os.path.join('dataset', 'custom_split_inv_clahe')\n",
    "    train, val = os.path.join(data_root, joint_type_and_param, 'train'), os.path.join(data_root, joint_type_and_param, 'test')\n",
    "\n",
    "    n_classes = max(len(os.listdir(train)), len(os.listdir(val)))\n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(train, train_transform)\n",
    "    val_dataset = torchvision.datasets.ImageFolder(val, val_transform)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    print(len(train_dataloader), len(val_dataloader))\n",
    "    return train_dataloader, val_dataloader, n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of images in a batch\n",
    "Currently isn't used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all images in a batch\n",
    "\n",
    "def show_input(input_tensor, title=''):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array( [0.229, 0.224, 0.225])\n",
    "\n",
    "    image = input_tensor.permute(1, 2, 0).numpy()\n",
    "    image = std * image + mean\n",
    "    plt.imshow(image.clip(0, 1))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "def visualise_batch(train_dataloader):\n",
    "    X_batch, y_batch = next(iter(train_dataloader))\n",
    "\n",
    "    for x_item, y_item in zip(X_batch, y_batch):\n",
    "        show_input(x_item, title={y_item})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of model training and prediction functions\n",
    "And 'test_time_augmentations' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, loss, optimizer, num_epochs, device, output_transform = None):\n",
    "    '''\n",
    "    Model training loop (num_epochs), every epoch has 2 phases: train and val.\n",
    "    Loss criterion: ordered cross entropy.\n",
    "    Backpropagation is only done on train phase.\n",
    "\n",
    "    First 20 epochs are for training of the last layer, on epoch 21 all other model parameters are set to have gradient.\n",
    "\n",
    "    Early stopping mechanism is implemented to return a model with smallest loss value on val data.\n",
    "\n",
    "    Args:\n",
    "        model\n",
    "        train_dataloader\n",
    "        val_dataloader\n",
    "        loss (Criterion)\n",
    "        optimizer (torch.optim.Adam)\n",
    "        num_epochs (int)\n",
    "        device (torch.device) - 'cuda:0' or 'cpu'\n",
    "        output_transform = None (str): 'bin' or 'minor'\n",
    "    \n",
    "    Return:\n",
    "        best_model\n",
    "    '''\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if epoch == 20:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                dataloader = train_dataloader\n",
    "                model.train()\n",
    "            else:\n",
    "                dataloader = val_dataloader\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "\n",
    "            y_preds, y_trues = [], []\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    preds = model(inputs)\n",
    "                    if output_transform:\n",
    "                       labels = out_transform(labels, output_transform)\n",
    "                    \n",
    "                    # calculate ordered CrossEntropy\n",
    "                    loss_value = loss(preds, labels)\n",
    "                    distance_weight = torch.abs(preds.argmax(1) - labels) + 1\n",
    "                    ordinal_ce_loss = torch.mean(distance_weight * loss_value)\n",
    "                    preds_class = preds.argmax(dim=1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        # backpropagation only in train phase\n",
    "                        ordinal_ce_loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else:\n",
    "                        y_preds.extend(preds_class.detach().cpu().tolist())\n",
    "                        y_trues.extend(labels.detach().cpu().tolist())\n",
    "\n",
    "                running_loss += loss_value.item()\n",
    "                running_acc += (preds_class == labels.data).float().mean()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_acc = running_acc / len(dataloader)\n",
    "\n",
    "            #early stopping based on model loss on val data\n",
    "            if phase == 'train':\n",
    "                train_acc = epoch_acc\n",
    "            else:\n",
    "                val_acc = epoch_acc\n",
    "                val_loss = epoch_loss\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "         #   print('Epoch {},\\n    train accuracy: {:.4f},\\n    val accuracy: {:.4f}, f1: {:.4f}'.format(epoch+1, train_acc, val_acc, \\\n",
    "          #                                                                                               f1_score(y_trues, y_preds, average='micro')), flush = True)\n",
    "\n",
    "        if epoch == 0:\n",
    "            best_loss = val_loss\n",
    "        elif val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    #print('Epoch {},\\n    train accuracy: {:.4f},\\n    val accuracy: {:.4f}, f1: {:.4f}'.format(epoch+1, train_acc, val_acc, \\\n",
    "     #                                                                                           f1_score(y_trues, y_preds, average='micro')), flush = True)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_augmentations(preds, inputs, model):\n",
    "    '''\n",
    "    Test time augmentations to increase accuracy on test data.\n",
    "    Used augmentations: rotation + and - 10 degrees, \n",
    "        horizontal flip of the original image and same rotations of flipped image.\n",
    "    \n",
    "    Args:\n",
    "        preds (torch.Tensor): model predictions for the original image (or batch of images)\n",
    "        inputs (torch.Tensor): original image (or batch of images)\n",
    "        model\n",
    "    \n",
    "    Return:\n",
    "        preds (torch.Tensor): sum of predictions for all augmentations\n",
    "    '''\n",
    "    inp_rot_p15 = v2.functional.rotate(inputs, 10)\n",
    "    inp_rot_m15 = v2.functional.rotate(inputs, -10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds += model(v2.functional.horizontal_flip(inputs))\n",
    "        preds += model(v2.functional.horizontal_flip(inp_rot_p15))\n",
    "        preds += model(v2.functional.horizontal_flip(inp_rot_m15))\n",
    "        preds += model(inp_rot_p15)\n",
    "        preds += model(inp_rot_m15)\n",
    "    return preds\n",
    "\n",
    "def predict_test(model, test_dataloader, device):\n",
    "    '''\n",
    "    Predict test class names.\n",
    "    Returns accuracy and f1-score using predictions and predictions with test time augmentations.\n",
    "\n",
    "    Args:\n",
    "        model\n",
    "        val_dataloader\n",
    "        device (torch.device): 'cuda' or 'cpu'\n",
    "    \n",
    "    Return:\n",
    "        acc (float): accuracy, ranging from 0 to 1\n",
    "        acc_tta (float): accuracy (with test time augmentations), ranging from 0 to 1\n",
    "        f1 (float): f1-score, ranging from 0 to 1\n",
    "        f1_tta (float): f1-score (with test time augmentations), ranging from 0 to 1\n",
    "    '''\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    acc_tta = 0\n",
    "\n",
    "    y_preds, y_preds_tta, y_trues = [], [], []\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        y_trues.extend(labels.detach().cpu().tolist())\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs)\n",
    "\n",
    "        y_pred = preds.argmax(dim=1)\n",
    "        y_preds.extend(y_pred.detach().cpu().tolist())\n",
    "        acc += (y_pred == labels.data).float().mean()\n",
    "\n",
    "        y_pred_tta = test_time_augmentations(preds, inputs, model).argmax(dim=1)\n",
    "        y_preds_tta.extend(y_pred_tta.detach().cpu().tolist())\n",
    "        acc_tta += (y_pred_tta == labels.data).float().mean()\n",
    "\n",
    "    f1 = f1_score(y_trues, y_preds, average='micro')\n",
    "    f1_tta = f1_score(y_trues, y_preds_tta, average='micro')\n",
    "    print('Test accuracy = {:.4f}, f1 = {:.4f}'.format(acc/len(test_dataloader), f1))\n",
    "    print('Test accuracy (with tta) = {:.4f}, f1 = {:.4f}'.format(acc_tta/len(test_dataloader), f1_tta))\n",
    "    return acc/len(test_dataloader), acc_tta/len(test_dataloader), f1, f1_tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_classes):\n",
    "    '''\n",
    "    Get models:\n",
    "        1. Load pretrained EfficientNet b4.\n",
    "        2. Disable gradient on all layers, except the last one.\n",
    "        3. Change last layer so the output size is equal to number of classes. \n",
    "        4. Get device ('cuda' or 'cpu') and load model to device.\n",
    "\n",
    "    Make loss and optimizer.\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): number of classes.\n",
    "\n",
    "    Return:\n",
    "        model\n",
    "        loss (torch.nn.CrossEntropyLoss)\n",
    "        optimizer (torch.optim.Adam)\n",
    "        device (torch.device): 'cuda' or 'cpu'\n",
    "    '''\n",
    "\n",
    "    model = models.efficientnet_b4(weights = models.EfficientNet_B4_Weights.DEFAULT)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = torch.nn.Sequential(torch.nn.Dropout(p = 0.2, inplace=True),\n",
    "                                        torch.nn.Linear(model.classifier[1].in_features, model.classifier[1].in_features//2),\n",
    "                                        torch.nn.Dropout(p = 0.2, inplace=True),\n",
    "                                        torch.nn.LeakyReLU(),\n",
    "                                        torch.nn.Linear(model.classifier[1].in_features//2, n_classes))\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using {device}')\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3.0e-4)\n",
    "\n",
    "    return model, loss, optimizer, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count which classes are balanced and train a model for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_disbalance(df, param):\n",
    "    d = {}\n",
    "    for column in df.columns:\n",
    "        vals = df[column].to_list()\n",
    "        s = sum(vals)\n",
    "        for i, v in enumerate(vals):\n",
    "            if v > s * .5:\n",
    "                d[f'{column}_{param}'] = str(i)\n",
    "                break\n",
    "        else:\n",
    "            d[f'{column}_{param}'] = 'balanced'\n",
    "    return d\n",
    "\n",
    "erosion_df = pd.read_csv(os.path.join('dataset', 'non-sorted', 'erosion_data_counts.csv'), header = 0, index_col=0)\n",
    "jsn_df = pd.read_csv(os.path.join('dataset', 'non-sorted', 'jsn_data_counts.csv'), header = 0, index_col=0)\n",
    "er_d, jsn_d = count_disbalance(erosion_df, 'erosion'), count_disbalance(jsn_df, 'jsn')\n",
    "print(er_d)\n",
    "print(jsn_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dict = dict(list(er_d.items()) + list(jsn_d.items()))\n",
    "\n",
    "for key in full_dict:\n",
    "    if full_dict[key] == 'balanced':\n",
    "        print(f'\\n {key}')\n",
    "        train_dataloader, val_dataloader, n_classes = get_dataloaders(key)\n",
    "        model, loss, optimizer, device = get_model(n_classes)\n",
    "        model = train_model(model, train_dataloader, val_dataloader, loss, optimizer, num_epochs = 100, device = device)\n",
    "\n",
    "        test_acc, test_acc_tta, f1, f1_tta = predict_test(model, val_dataloader, device)\n",
    "        #check for model name\n",
    "        torch.save(model, os.path.join('models', \\\n",
    "                        'effNetb4_{}_{:.3f}_tta_{:.3f}.json'.format(key, test_acc, test_acc_tta)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
